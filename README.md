# ğŸš€ Gemini: Carga Masiva CSV a BigQuery con Apache Beam Dataflow

## ğŸ¯ **Â¿QuÃ© hace este proyecto?**

SoluciÃ³n optimizada para cargar archivos CSV masivos a BigQuery usando **Apache Beam Dataflow**, aprovechando el procesamiento distribuido de Google Cloud para mÃ¡xima velocidad y escalabilidad.

---

## ğŸš€ **CaracterÃ­sticas principales:**

- âœ… **Procesamiento distribuido** con Apache Beam Dataflow
- âœ… **Escalabilidad automÃ¡tica** hasta 100 workers
- âœ… **Manejo de archivos grandes** sin lÃ­mites de tamaÃ±o
- âœ… **ConfiguraciÃ³n optimizada** para mÃ¡xima velocidad
- âœ… **Monitoreo en tiempo real** del progreso
- âœ… **Limpieza automÃ¡tica** de recursos temporales

---

## ğŸ“Š **Ventajas vs mÃ©todos tradicionales:**

| MÃ©todo | Tiempo | Escalabilidad | Costo |
|--------|--------|----------------|-------|
| **BigQuery Load** | 2-5 horas | âŒ Limitada | ğŸ’° Bajo |
| **Dataflow (Gemini)** | **30-90 min** | âœ… **AutomÃ¡tica** | ğŸ’° **Medio** |
| **Streaming** | 1-2 horas | âœ… Alta | ğŸ’° Alto |

---

## ğŸ› ï¸ **Requisitos:**

- **Proyecto Google Cloud** con facturaciÃ³n habilitada
- **Cloud Shell** o terminal con gcloud configurado
- **Permisos** para Dataflow, BigQuery y Cloud Storage
- **Archivo CSV** en Cloud Storage

---

## ğŸš€ **InstalaciÃ³n y uso:**

### **1. Ejecutar setup automÃ¡tico:**
```bash
# Clonar o descargar los archivos
cd gemini

# Ejecutar setup automÃ¡tico
bash setup_gemini.sh
```

### **2. ConfiguraciÃ³n manual (alternativa):**
```bash
# Configurar proyecto
gcloud config set project acero-470020

# Habilitar APIs
gcloud services enable dataflow.googleapis.com compute.googleapis.com bigquery.googleapis.com

# Crear bucket
gsutil mb gs://acero_bucket

# Crear dataset
bq mk --dataset acero-470020:dataset_acero
```

---

## ğŸ“ **Estructura del proyecto:**

```
gemini/
â”œâ”€â”€ README.md                    # Este archivo
â”œâ”€â”€ setup_gemini.sh             # Script de configuraciÃ³n automÃ¡tica
â”œâ”€â”€ load_to_bq.py               # Pipeline principal de Dataflow
â”œâ”€â”€ requirements.txt             # Dependencias de Python
â”œâ”€â”€ config.py                   # ConfiguraciÃ³n del proyecto
â”œâ”€â”€ QUICK_START.md              # GuÃ­a de inicio rÃ¡pido
â””â”€â”€ MONITORING.md               # GuÃ­a de monitoreo
```

---

## ğŸ”§ **ConfiguraciÃ³n personalizada:**

Edita las variables en `config.py`:

```python
PROJECT_ID = "acero-470020"
BUCKET = "acero_bucket"
DATASET = "dataset_acero"
TABLE = "acero_table"
GCS_FILE_PATH = "gs://desafio-deacero-143d30a0-d8f8-4154-b7df-1773cf286d32/cdo_challenge.csv.gz"
```

---

## ğŸ“ˆ **Monitoreo y seguimiento:**

### **Ver progreso en tiempo real:**
```bash
# Ver jobs de Dataflow
gcloud dataflow jobs list

# Monitorear job especÃ­fico
gcloud dataflow jobs show [JOB_ID]

# Ver logs del job
gcloud dataflow jobs logs [JOB_ID]
```

---

## ğŸš¨ **SoluciÃ³n de problemas:**

### **Error: "API not enabled"**
- âœ… **SoluciÃ³n**: Ejecutar `gcloud services enable dataflow.googleapis.com`

### **Error: "Insufficient quota"**
- âœ… **SoluciÃ³n**: Reducir `num_workers` en el script

### **Error: "Bucket not found"**
- âœ… **SoluciÃ³n**: Crear bucket con `gsutil mb gs://acero_bucket`

---

## ğŸ’¡ **Consejos para mÃ¡xima velocidad:**

1. **Usar regiones cercanas** a tu ubicaciÃ³n
2. **Ajustar workers** segÃºn el tamaÃ±o del archivo
3. **Monitorear recursos** durante la ejecuciÃ³n
4. **Verificar permisos** antes de ejecutar

---

## ğŸ‰ **Â¡Resultado esperado!**

- âœ… **Tiempo total**: 30-90 minutos (vs 2-5 horas)
- âœ… **Escalabilidad**: AutomÃ¡tica hasta 100 workers
- âœ… **Confiabilidad**: Procesamiento distribuido robusto
- âœ… **Monitoreo**: Seguimiento en tiempo real

---

## ğŸš€ **Â¡De 5 horas a 1 hora!**

Con Apache Beam Dataflow, tu archivo CSV masivo estarÃ¡ en BigQuery en **30-90 minutos** con escalabilidad automÃ¡tica.

**Â¡La diferencia es abismal! ğŸš€âš¡**
